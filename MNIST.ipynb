{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11646bc",
   "metadata": {},
   "source": [
    "#**MNIST- refers to image identification of hand written digits data**\n",
    "\n",
    "**This project focuses on creating a neural netwrok model completly using numpy instead pof tensorflow library**\n",
    "\n",
    "As for the data set we can either get it through kaggle, keras, or even openml\n",
    "In this project we are using openML since we do not want to use keras in this complete code\n",
    "\n",
    "##Some conventions\n",
    "Upper case letters refres to 2D matrix datas. Ex- shape( samples, features )\n",
    "Lower case refrs to 1D vectors. Ex- shape(label)\n",
    "\n",
    "##Forward Propagation\n",
    "Each layer has an forward prop where it sends data to the next layer after getting output from the uploaded input, goes from first layer to end\n",
    "#Backward Propagation\n",
    "works from last layer to first where the derivatives are sent backward to update the weights and bias and sent again until loss is decreased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000d7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96079a59",
   "metadata": {},
   "source": [
    "#Load and seperate data for train and test\n",
    "\n",
    "The data will be a 28x28 pixel image's greyscale intensity data (0-255 for each pixel) for each number which is filled with  black to white colors, which we refer as 0-1. Where 0 refers to black and 1 refers to white, but in system recoginition the value is given as its density value which is 0 for black and 255 for white, hence to use the data set we have to do these steps\n",
    "\n",
    "First, the openml data is given as \n",
    "X is a float of 784 (28x28) values per row that is 784 features for each sample, and y is a list of strings - ('0','1',2'....,'9')\n",
    "hence we convert y label values to int from string\n",
    "then divide then x data by 255 to get values between 0 and 1.\n",
    "(x value is color that is 01 and y value is 0-9 number labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3a07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784), Test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST data from openML\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "#Convert string to int\n",
    "y = y.astype(np.int64)\n",
    "#Normalize pixel value that is 0 to 255 is changed to 0-1\n",
    "X = X / 255.0\n",
    "#split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42, stratify=y)\n",
    "#In above, random_state=42 refers to random splitting for better working, the 42 can be any number\n",
    "# stratify = y is used to arrange y values such that it is not too biased in either test or train set ( that is the same label (0-9) is spread evenly in percentage)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "#this data set has 60k sample each of 784px hence those many features for each, test data set is 10k as given above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c5b9",
   "metadata": {},
   "source": [
    "#Batch Generator\n",
    "###Neural network do not train on the entire dataset at once - too slow and too much memory\n",
    "INSTEAD WE USE MINI BATCHES, taht is train 32-64 data per batch and then take another batch while removing the previous batch from emory - this helps when the dataset is large.\n",
    "\n",
    "ex: when we have 60k images we train 32 to 64 images per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415c86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=32):\n",
    "    n_samples = X.shape[0] #total number of samples\n",
    "    indices = np.arange(n_samples)   # this creates a list with the specified number of terms like [0,1,2,3,...]\n",
    "    np.random.shuffle(indices)       # shuffle the list for randomness\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_ids = indices[start:end]\n",
    "        yield X[batch_ids], y[batch_ids] #yield is used such that memory is saved since it outputs one step then forgets memory and does it for the next batch unlike how return does is send it as a largee single block\n",
    "#for those with doubts how X[[23,45,67,32,43,..]] this type of data gives output - this works as X[23],X[45],... note that this is how np.array works and will noyt work with python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ce403",
   "metadata": {},
   "source": [
    "#Layer Dense\n",
    "\n",
    "For this we will be using class instead of direct functions\n",
    "\n",
    "##For those who do not have any basic idea of class-\n",
    "in simple words it is where we group multiple functions all having connected variable for each object (child of the class) created/initialized\n",
    "the first function called __init__ runs eventhough it is not called whenever an object is defined, meaning it initialises the data such as the basic variables that are used by that object's functions\n",
    "the functions inside the object are accessed by '.' operator\n",
    "\n",
    "##Why use a class for dense\n",
    "A Dense (fully connected) layer needs:\n",
    "- A set of **weights** and **biases** for each layer.\n",
    "- A **forward method** to compute outputs.\n",
    "- A **backward method** to compute gradients (for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1acc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object = DenseLayer(n_iputs, n_neurons) these parameter values are the __init__ function's parameter\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        A fully connected (Dense) layer.\n",
    "        n_inputs: number of features (e.g. 784 for MNIST)\n",
    "        n_neurons: number of neurons/units in this layer\n",
    "        \"\"\"\n",
    "        # Initialize weights small random values\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)#creates a matrix of random numbers as value which is multipled by 0.01 to reduce its size, the matrix shape is (n_iputs,n_neurons)\n",
    "        # matrix shape here means that each of the n_neurons have n_inputs number of weights\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons)) #creates a array of zeroes for the guven shape (1,number of units in the layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        inputs: shape (batch_size, n_inputs)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs  # keep for backward prop, input refers to the acivataion 'a vector' passed to this layer by the previous layer \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):#d hear means symbol for differentiation\n",
    "        # dvalues: gradient from the next layer (‚àÇL/‚àÇoutput), \n",
    "        # similar to \"y_hat - y\" (that only shows up at the loss layer).\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)   # ‚àÇL/‚àÇW, .T means transpose, where the value of diff wrt w is summation of (y hat - y )* x_i\n",
    "        # Each weight‚Äôs gradient is how much changing that weight changes the loss\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)  # ‚àÇL/‚àÇb, the value of diff wrt b is summation of (y cap - y )\n",
    "        # Gradient w.r.t. inputs: dL/dout ¬∑ W·µÄ\n",
    "        # This passes the gradient backward so earlier layers can update too    \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)  \n",
    "        #and the prevoius layer uses these dvalues and updates its weight and bias \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a079dfa",
   "metadata": {},
   "source": [
    "#Layer - ReLu Activation\n",
    "\n",
    "ReLu function g(x)=max(0,x)\n",
    "We store the input for later add ons such as backward propagation\n",
    "##Backward prop\n",
    "‚àÇùêø/‚àÇùë¶ = gradient coming from the next layer, that‚Äôs what you called dvalues or doutput.\n",
    "‚àÇùë¶/‚àÇùë• = derivative of the activation function (e.g., ReLU).\n",
    "‚àÇùêø/‚àÇùë• = gradient wrt the input (what we need to send further back) that‚Äôs dinputs, which is a product of the first two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs for backprop\n",
    "        self.inputs = inputs\n",
    "        # Relu function is max(0,x)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()#copy values of the next layer's gradient/derivative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        '''\n",
    "        inputs  = [-2, -1,  0,  3,  5] output for this [0,0,0,3,5] as per Relu\n",
    "        dvalues = [ 1,  2,  3,  4,  5] this is the derivative or gradient passed from the next layer\n",
    "        dinputs = [ 0,  0,  0,  4,  5] wherever the input is negative the dinput is also zero, and whenevr it is positive we get the dvalue\n",
    "        relu = f(x)= 0 if x is <=0, f(x)=x, x>0\n",
    "        derivative of f(x) = 0 if x is <=0, derivative of f(x)=1, x>0\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3a6e0",
   "metadata": {},
   "source": [
    "#Layer - Softmax Activation\n",
    "\n",
    "Softmax - is where the input gives a output between 0 to 1 (probabilities that sum up to 1)with the formula e^zi/sum of e^zis, here the input is the 'activation 'a' matrix of vectors't we get as output from the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9ef954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSoftmax:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs for backprop\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # inputs is a matrix of multiple samples with multiple features in a row, here np.exp also ceate the matrix of same dimension but with gtheir exponent value as the output\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        #above we subtract each value in each row of input by the max value in the input's each row so that if the values are too learge like 1000 then e^1000 will be too big so we normalize it \n",
    "        \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        #axis=1, refers to row wise sums that divide each value in that row in the matrix\n",
    "        \n",
    "        self.output = probabilities\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Rarely used directly since we combine with loss\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0556bed",
   "metadata": {},
   "source": [
    "#Layer Loss CategoricalCrossEntropy\n",
    "\n",
    "Used to calculate loss of Softmax activation, calculated by 1/N * sum of (log(prediction_i))) that is mean formula\n",
    "for a digit (like '2') the y_true can be [0,0,1,0,0,0,0,0,0,0] called one hot encoding or [0,1,2,3,4,5,6,7,8,9] \n",
    "\n",
    "##Example with 3 digits input [0,1,2]\n",
    "y_pred = np.array([\n",
    "    [0.1, 0.7, 0.2],   ### model says: \"likely class 1\"\n",
    "    [0.8, 0.1, 0.1],   ### model says: \"likely class 0\"\n",
    "    [0.2, 0.2, 0.6]    ### model says: \"likely class 2\"\n",
    "])\n",
    "### shape = (3 samples, 3 classes)\n",
    "True labels (y_true):\n",
    "Case A ‚Üí as integers:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "y_true = np.array([1, 0, 2])   ### correct classes are 1, 0, 2\n",
    "### shape = (3,)\n",
    "Case B ‚Üí one-hot encoded:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "y_true = np.array([\n",
    "    [0, 1, 0],   ### class 1\n",
    "    [1, 0, 0],   ### class 0\n",
    "    [0, 0, 1]    ### class 2\n",
    "])\n",
    "### shape = (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1feb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # clip values to prevent log(0), that is the value is in range 1^-7 to 1 - 1^-7\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # if labels are one-hot encoded [0,0,1,0,0,0,0,0,0,0],[....],[...] means class '2' in first row, and like this for each sample, like this for each sample\n",
    "        if len(y_true.shape) == 2:#since it is 2D matrix with shape like (10,10)- 10 rows and 10 column , and the length of (10,10) is 2\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # if labels are just class indices [1,0,2...] where number of terms is same as number of samples, and the number is the integer that has the most probability for each sample\n",
    "        elif len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true] #range(3)= [0,1,2], here y_pr....[..,..]=[[0,1,2],[1,0,2]]=[[0,1],[1,0],[2,2]]\n",
    "\n",
    "        # loss = negative log likelihood\n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(neg_log_likelihoods) #1/N sum of logs\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)       # number of samples in the batch\n",
    "        labels = len(dvalues[0])     # number of classes (labels)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            #np.eye creates diagonal matrix [1,0,0],[0,1,0],[0,0,1]\n",
    "            #[y_true] whose value are the labels that are most propable like (0,2) chooses that row of the diagonal matrix to change into one hot encoding \n",
    "            #np.eye(3)[y_true] makes [1,2] meaning the first propability hiogh for number label 1 and next is for 2 is converted to one hot encoding ([[0,1,0],[0,0,1]])\n",
    "        self.dinputs = -y_true / dvalues#‚àÇL/‚àÇy_pred = -y_true/y_pred\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a78917",
   "metadata": {},
   "source": [
    "#Optimizer_Adam (complex mathematical formulas for beginner level, better to use the library rather than confuse yourselves)\n",
    "As for definition an optimizer helps reduce the number of steops to reach minima by adjusting the learning rate so that it does not diverge but meets the minima at a faster rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0   # to keep track of steps\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        # If first time, initialize momentums and caches\n",
    "        if not hasattr(layer, 'm_w'):\n",
    "            layer.m_w = np.zeros_like(layer.weights)\n",
    "            layer.v_w = np.zeros_like(layer.weights)\n",
    "            layer.m_b = np.zeros_like(layer.biases)\n",
    "            layer.v_b = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update step count\n",
    "        self.iterations += 1\n",
    "\n",
    "        # Momentum update for weights and biases\n",
    "        layer.m_w = self.beta1 * layer.m_w + (1 - self.beta1) * layer.dweights\n",
    "        layer.m_b = self.beta1 * layer.m_b + (1 - self.beta1) * layer.dbiases\n",
    "\n",
    "        # RMSProp cache update\n",
    "        layer.v_w = self.beta2 * layer.v_w + (1 - self.beta2) * (layer.dweights ** 2)\n",
    "        layer.v_b = self.beta2 * layer.v_b + (1 - self.beta2) * (layer.dbiases ** 2)\n",
    "\n",
    "        # Correct bias for small t\n",
    "        m_w_corr = layer.m_w / (1 - self.beta1 ** self.iterations)\n",
    "        m_b_corr = layer.m_b / (1 - self.beta1 ** self.iterations)\n",
    "        v_w_corr = layer.v_w / (1 - self.beta2 ** self.iterations)\n",
    "        v_b_corr = layer.v_b / (1 - self.beta2 ** self.iterations)\n",
    "\n",
    "        # Update parameters\n",
    "        layer.weights -= self.learning_rate * m_w_corr / (np.sqrt(v_w_corr) + self.epsilon)\n",
    "        layer.biases  -= self.learning_rate * m_b_corr / (np.sqrt(v_b_corr) + self.epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
