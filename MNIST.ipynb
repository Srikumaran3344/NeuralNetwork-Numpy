{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11646bc",
   "metadata": {},
   "source": [
    "#**MNIST- refers to image identification of hand written digits data**\n",
    "\n",
    "**This project focuses on creating a neural netwrok model completly using numpy instead pof tensorflow library**\n",
    "\n",
    "As for the data set we can either get it through kaggle, keras, or even openml\n",
    "In this project we are using openML since we do not want to use keras in this complete code\n",
    "\n",
    "##Some conventions\n",
    "Upper case letters refres to 2D matrix datas. Ex- shape( samples, features )\n",
    "Lower case refrs to 1D vectors. Ex- shape(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000d7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96079a59",
   "metadata": {},
   "source": [
    "#Load and seperate data for train and test\n",
    "\n",
    "The data will be a 28x28 pixel image's greyscale intensity data (0-255 for each pixel) for each number which is filled with  black to white colors, which we refer as 0-1. Where 0 refers to black and 1 refers to white, but in system recoginition the value is given as its density value which is 0 for black and 255 for white, hence to use the data set we have to do these steps\n",
    "\n",
    "First, the openml data is given as \n",
    "X is a float of 784 (28x28) values per row that is 784 features for each sample, and y is a list of strings - ('0','1',2'....,'9')\n",
    "hence we convert y label values to int from string\n",
    "then divide then x data by 255 to get values between 0 and 1.\n",
    "(x value is color that is 01 and y value is 0-9 number labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3a07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784), Test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST data from openML\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "#Convert string to int\n",
    "y = y.astype(np.int64)\n",
    "#Normalize pixel value that is 0 to 255 is changed to 0-1\n",
    "X = X / 255.0\n",
    "#split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42, stratify=y)\n",
    "#In above, random_state=42 refers to random splitting for better working, the 42 can be any number\n",
    "# stratify = y is used to arrange y values such that it is not too biased in either test or train set ( that is the same label (0-9) is spread evenly in percentage)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "#this data set has 60k sample each of 784px hence those many features for each, test data set is 10k as given above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c5b9",
   "metadata": {},
   "source": [
    "#Batch Generator\n",
    "###Neural network do not train on the entire dataset at once - too slow and too much memory\n",
    "INSTEAD WE USE MINI BATCHES, taht is train 32-64 data per batch and then take another batch while removing the previous batch from emory - this helps when the dataset is large.\n",
    "\n",
    "ex: when we have 60k images we train 32 to 64 images per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=32):\n",
    "    n_samples = X.shape[0] #total number of samples\n",
    "    indices = np.arange(n_samples)   # this creates a list with the specified number of terms like [0,1,2,3,...]\n",
    "    np.random.shuffle(indices)       # shuffle the list for randomness\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_ids = indices[start:end]\n",
    "        yield X[batch_ids], y[batch_ids] #yield is used such that memory is saved since it outputs one step then forgets memory and does it for the next batch unlike how return does is send it as a largee single block\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
