{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11646bc",
   "metadata": {},
   "source": [
    "#**MNIST- refers to image identification of hand written digits data**\n",
    "\n",
    "**This project focuses on creating a neural netwrok model completly using numpy instead pof tensorflow library**\n",
    "\n",
    "As for the data set we can either get it through kaggle, keras, or even openml\n",
    "In this project we are using openML since we do not want to use keras in this complete code\n",
    "\n",
    "##Some conventions\n",
    "Upper case letters refres to 2D matrix datas. Ex- shape( samples, features )\n",
    "Lower case refrs to 1D vectors. Ex- shape(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000d7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96079a59",
   "metadata": {},
   "source": [
    "#Load and seperate data for train and test\n",
    "\n",
    "The data will be a 28x28 pixel image's greyscale intensity data (0-255 for each pixel) for each number which is filled with  black to white colors, which we refer as 0-1. Where 0 refers to black and 1 refers to white, but in system recoginition the value is given as its density value which is 0 for black and 255 for white, hence to use the data set we have to do these steps\n",
    "\n",
    "First, the openml data is given as \n",
    "X is a float of 784 (28x28) values per row that is 784 features for each sample, and y is a list of strings - ('0','1',2'....,'9')\n",
    "hence we convert y label values to int from string\n",
    "then divide then x data by 255 to get values between 0 and 1.\n",
    "(x value is color that is 01 and y value is 0-9 number labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3a07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784), Test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST data from openML\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "#Convert string to int\n",
    "y = y.astype(np.int64)\n",
    "#Normalize pixel value that is 0 to 255 is changed to 0-1\n",
    "X = X / 255.0\n",
    "#split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42, stratify=y)\n",
    "#In above, random_state=42 refers to random splitting for better working, the 42 can be any number\n",
    "# stratify = y is used to arrange y values such that it is not too biased in either test or train set ( that is the same label (0-9) is spread evenly in percentage)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "#this data set has 60k sample each of 784px hence those many features for each, test data set is 10k as given above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c5b9",
   "metadata": {},
   "source": [
    "#Batch Generator\n",
    "###Neural network do not train on the entire dataset at once - too slow and too much memory\n",
    "INSTEAD WE USE MINI BATCHES, taht is train 32-64 data per batch and then take another batch while removing the previous batch from emory - this helps when the dataset is large.\n",
    "\n",
    "ex: when we have 60k images we train 32 to 64 images per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=32):\n",
    "    n_samples = X.shape[0] #total number of samples\n",
    "    indices = np.arange(n_samples)   # this creates a list with the specified number of terms like [0,1,2,3,...]\n",
    "    np.random.shuffle(indices)       # shuffle the list for randomness\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_ids = indices[start:end]\n",
    "        yield X[batch_ids], y[batch_ids] #yield is used such that memory is saved since it outputs one step then forgets memory and does it for the next batch unlike how return does is send it as a largee single block\n",
    "#for those with doubts how X[[23,45,67,32,43,..]] this type of data gives output - this works as X[23],X[45],... note that this is how np.array works and will noyt work with python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ce403",
   "metadata": {},
   "source": [
    "#Layer Dense\n",
    "\n",
    "For this we will be using class instead of direct functions\n",
    "\n",
    "##For those who do not have any basic idea of class-\n",
    "in simple words it is where we group multiple functions all having connected variable for each object (child of the class) created/initialized\n",
    "the first function called __init__ runs eventhough it is not called whenever an object is defined, meaning it initialises the data such as the basic variables that are used by that object's functions\n",
    "the functions inside the object are accessed by '.' operator\n",
    "\n",
    "##Why use a class for dense\n",
    "A Dense (fully connected) layer needs:\n",
    "- A set of **weights** and **biases** for each layer.\n",
    "- A **forward method** to compute outputs.\n",
    "- A **backward method** to compute gradients (for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1acc43",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 5 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(y[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m45\u001b[39m,\u001b[38;5;241m67\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 5 were indexed"
     ]
    }
   ],
   "source": [
    "#object = DenseLayer(n_iputs, n_neurons) these parameter values are the __init__ function's parameter\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        A fully connected (Dense) layer.\n",
    "        n_inputs: number of features (e.g. 784 for MNIST)\n",
    "        n_neurons: number of neurons/units in this layer\n",
    "        \"\"\"\n",
    "        # Initialize weights small random values\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)#creates a matrix of random numbers as value which is multipled by 0.01 to reduce its size, the matrix shape is (n_iputs,n_neurons)\n",
    "        # matrix shape is (784 features which has one weight each,number of units in this layer)\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons)) #creates a array of zeroes for the guven shape (1,number of units in the layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        inputs: shape (batch_size, n_inputs)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs  # keep for backward pass later\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
